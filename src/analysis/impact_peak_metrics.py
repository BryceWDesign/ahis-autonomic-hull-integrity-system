#!/usr/bin/env python3
"""
AHIS PoC Analysis — Impact Peak Metrics

Purpose
-------
Compute peak metrics (e.g., peak strain µε and/or peak acceleration g) from one or more
raw CSV time-series files captured during low-velocity impact tests (T-IMP-010).

This script is intentionally strict:
- It does not guess units.
- It requires you to specify which columns represent time and which represent metrics.
- It produces a machine-auditable summary CSV (and optional grouped stats if you provide a map).

Expected Raw Data
-----------------
One or more CSV files where each file contains time-series columns.

You must provide:
- a time column name (default: "time_s")
- one or more metric column names (e.g., "strain_ue", "accel_g")

Column naming is your responsibility; if your files differ, use CLI args.

Outputs
-------
- processed/impact_peak_summary.csv
- processed/impact_peak_group_stats.csv (only if --map is provided)

No plots are generated by default (PoC hygiene). Plotting can be added later once real data exists.

Usage Examples
--------------
1) Compute peaks for strain and acceleration columns:
   python3 impact_peak_metrics.py \
     --input results/T-IMP-010/RUN_YYYY-MM-DD_XYZ/raw \
     --output results/T-IMP-010/RUN_YYYY-MM-DD_XYZ/processed \
     --time-col time_s \
     --metric strain_ue \
     --metric accel_g

2) Add grouping (baseline vs AHIS) using a mapping CSV:
   python3 impact_peak_metrics.py \
     --input <raw_dir> \
     --output <processed_dir> \
     --time-col time_s \
     --metric strain_ue \
     --map impact_file_groups.csv

Where impact_file_groups.csv contains:
   filename,group
   hit1.csv,baseline
   hit2.csv,baseline
   hit3.csv,ahis
   hit4.csv,ahis
"""

from __future__ import annotations

import argparse
import csv
import math
import os
from dataclasses import dataclass
from typing import Dict, List, Optional, Sequence, Tuple


@dataclass(frozen=True)
class PeakResult:
    filename: str
    metric: str
    peak_value: float
    peak_abs_value: float
    t_at_peak_s: float
    n_rows: int


def _read_csv_rows(path: str) -> Tuple[List[str], List[Dict[str, str]]]:
    with open(path, "r", newline="", encoding="utf-8") as f:
        reader = csv.DictReader(f)
        if reader.fieldnames is None:
            raise ValueError(f"CSV has no header row: {path}")
        rows: List[Dict[str, str]] = []
        for row in reader:
            # Keep raw strings; parse on demand to allow strict errors.
            rows.append(row)
        return reader.fieldnames, rows


def _parse_float(value: str, *, path: str, col: str, row_idx: int) -> float:
    try:
        return float(value)
    except Exception as e:
        raise ValueError(
            f"Non-numeric value in {path} at row {row_idx+2} col '{col}': {value!r}"
        ) from e


def _compute_peak_for_metric(
    path: str,
    time_col: str,
    metric_col: str,
) -> PeakResult:
    headers, rows = _read_csv_rows(path)

    if time_col not in headers:
        raise ValueError(f"Missing time column '{time_col}' in {path}. Found: {headers}")
    if metric_col not in headers:
        raise ValueError(f"Missing metric column '{metric_col}' in {path}. Found: {headers}")

    if len(rows) == 0:
        raise ValueError(f"No data rows in {path}")

    best_abs = -1.0
    best_val = 0.0
    best_t = 0.0

    for i, row in enumerate(rows):
        t = _parse_float(row[time_col], path=path, col=time_col, row_idx=i)
        v = _parse_float(row[metric_col], path=path, col=metric_col, row_idx=i)
        av = abs(v)
        if av > best_abs:
            best_abs = av
            best_val = v
            best_t = t

    return PeakResult(
        filename=os.path.basename(path),
        metric=metric_col,
        peak_value=best_val,
        peak_abs_value=best_abs,
        t_at_peak_s=best_t,
        n_rows=len(rows),
    )


def _list_csv_files(input_dir: str) -> List[str]:
    if not os.path.isdir(input_dir):
        raise ValueError(f"Input path is not a directory: {input_dir}")

    files = []
    for name in os.listdir(input_dir):
        if name.lower().endswith(".csv"):
            files.append(os.path.join(input_dir, name))

    files.sort()
    if not files:
        raise ValueError(f"No .csv files found in input directory: {input_dir}")
    return files


def _load_group_map(map_path: str) -> Dict[str, str]:
    """
    Map file must contain header: filename,group
    filename should match the CSV basename in the input directory.
    """
    headers, rows = _read_csv_rows(map_path)
    required = ["filename", "group"]
    for r in required:
        if r not in headers:
            raise ValueError(f"Group map missing required column '{r}' in {map_path}. Found: {headers}")

    mapping: Dict[str, str] = {}
    for i, row in enumerate(rows):
        fn = (row.get("filename") or "").strip()
        grp = (row.get("group") or "").strip()
        if not fn or not grp:
            raise ValueError(f"Empty filename/group in {map_path} at data row {i+2}")
        mapping[fn] = grp
    return mapping


def _mean(xs: Sequence[float]) -> float:
    if not xs:
        raise ValueError("Cannot compute mean of empty list")
    return sum(xs) / float(len(xs))


def _std_sample(xs: Sequence[float]) -> float:
    """
    Sample standard deviation (n-1). Returns 0.0 for n<2 by convention.
    """
    n = len(xs)
    if n < 2:
        return 0.0
    m = _mean(xs)
    var = sum((x - m) ** 2 for x in xs) / float(n - 1)
    return math.sqrt(var)


def _write_summary_csv(out_path: str, peaks: List[PeakResult]) -> None:
    os.makedirs(os.path.dirname(out_path), exist_ok=True)
    with open(out_path, "w", newline="", encoding="utf-8") as f:
        writer = csv.writer(f)
        writer.writerow(["filename", "metric", "peak_value", "peak_abs_value", "t_at_peak_s", "n_rows"])
        for p in peaks:
            writer.writerow([p.filename, p.metric, p.peak_value, p.peak_abs_value, p.t_at_peak_s, p.n_rows])


def _write_group_stats_csv(out_path: str, peaks: List[PeakResult], group_map: Dict[str, str]) -> None:
    """
    Group stats are computed on peak_abs_value (magnitude) by default.
    This avoids sign confusion for strain gauge orientations.
    If you want signed peaks, change this intentionally and document it.
    """
    grouped: Dict[Tuple[str, str], List[float]] = {}  # (group, metric) -> values
    missing = []

    for p in peaks:
        grp = group_map.get(p.filename)
        if grp is None:
            missing.append(p.filename)
            continue
        key = (grp, p.metric)
        grouped.setdefault(key, []).append(p.peak_abs_value)

    if missing:
        # Strict: missing mappings mean your group stats would be misleading.
        raise ValueError(
            "Group map missing entries for these files: "
            + ", ".join(sorted(set(missing)))
        )

    os.makedirs(os.path.dirname(out_path), exist_ok=True)
    with open(out_path, "w", newline="", encoding="utf-8") as f:
        writer = csv.writer(f)
        writer.writerow(["group", "metric", "n", "mean_peak_abs", "std_peak_abs_sample"])
        for (grp, metric), values in sorted(grouped.items()):
            writer.writerow([grp, metric, len(values), _mean(values), _std_sample(values)])


def main() -> int:
    ap = argparse.ArgumentParser(description="Compute AHIS T-IMP-010 impact peak metrics from raw CSV time series.")
    ap.add_argument("--input", required=True, help="Directory containing raw CSV files for a run.")
    ap.add_argument("--output", required=True, help="Directory where processed outputs will be written.")
    ap.add_argument("--time-col", default="time_s", help="Name of the time column (seconds). Default: time_s")
    ap.add_argument(
        "--metric",
        action="append",
        required=True,
        help="Metric column name to compute peaks for. Can be provided multiple times.",
    )
    ap.add_argument(
        "--map",
        default=None,
        help="Optional CSV mapping file with columns: filename,group for grouped stats.",
    )

    args = ap.parse_args()
    input_dir: str = args.input
    out_dir: str = args.output
    time_col: str = args.time_col
    metrics: List[str] = args.metric
    map_path: Optional[str] = args.map

    csv_files = _list_csv_files(input_dir)

    peaks: List[PeakResult] = []
    for path in csv_files:
        for metric in metrics:
            peaks.append(_compute_peak_for_metric(path, time_col=time_col, metric_col=metric))

    summary_path = os.path.join(out_dir, "impact_peak_summary.csv")
    _write_summary_csv(summary_path, peaks)

    if map_path is not None:
        group_map = _load_group_map(map_path)
        group_stats_path = os.path.join(out_dir, "impact_peak_group_stats.csv")
        _write_group_stats_csv(group_stats_path, peaks, group_map)

    return 0


if __name__ == "__main__":
    raise SystemExit(main())
